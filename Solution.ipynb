{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weGQptNDRQQ4"
      },
      "source": [
        "# Random-forest & LSTM mixture approach\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrbB5f5LCvSc"
      },
      "source": [
        "Author: Chen Zhixin  \n",
        "Time: 2023/5/7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0J0aUvpkqr6"
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py8_06QWR1Ps"
      },
      "source": [
        "### 1.1 Import CSV file & Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxI-zsTK_4ZU"
      },
      "source": [
        "In the original data set, some blocks contains illegal values. Therefore, we need to clean them up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHkKwK48ktvf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"drive/MyDrive/flights.csv\",low_memory = False)\n",
        "\n",
        "data = data[data['MONTH'] == 1]\n",
        "\n",
        "# Convert the column \"name\" to a string type\n",
        "data[\"ORIGIN_AIRPORT\"] = data[\"ORIGIN_AIRPORT\"].astype(str)\n",
        "\n",
        "# Use the `apply()` method to check if each row in the column \"name\" is an integer\n",
        "is_integer = data[\"ORIGIN_AIRPORT\"].apply(lambda x: x.isdigit())\n",
        "\n",
        "# Use the `drop()` method to delete the rows where the values in the selected columns are integers.\n",
        "data = data.drop(data[is_integer].index)\n",
        "\n",
        "# Modify the columns of the DataFrame\n",
        "data_new = data.loc[:, ['MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','SCHEDULED_DEPARTURE','ARRIVAL_DELAY']]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvp3BJwBZR1y"
      },
      "source": [
        "### 1.2 Label encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuSlz40iZVBO"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the LabelEncoder object to the column\n",
        "le.fit(data_new[\"ORIGIN_AIRPORT\"])\n",
        "\n",
        "# Transform the column\n",
        "data_new[\"ORIGIN_AIRPORT\"] = le.transform(data_new[\"ORIGIN_AIRPORT\"])\n",
        "data_new[\"DESTINATION_AIRPORT\"] = le.transform(data_new[\"DESTINATION_AIRPORT\"])\n",
        "\n",
        "print(data_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds6G2nE-R9Fu"
      },
      "source": [
        "### 1.3 One-hot encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXf3-A9-ghKC"
      },
      "outputs": [],
      "source": [
        "# Use the `get_dummies()` method to one-hot encode the column\n",
        "data_new = pd.get_dummies(data_new, columns=['AIRLINE'])\n",
        "print(data_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb--toxWFbzO"
      },
      "source": [
        "### 1.4 Check the quality of the testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R53IBTayFfwv"
      },
      "outputs": [],
      "source": [
        "data_new = data_new.dropna(axis=0, how='any')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1gCapPHcULF"
      },
      "source": [
        "### 1.5 Norminazation (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcMElC9BcXAl"
      },
      "outputs": [],
      "source": [
        "# Normalize the 'a' column\n",
        "keyword = 'MONTH'\n",
        "maximun = data_new[keyword].max()\n",
        "minimun = data_new[keyword].min()\n",
        "data_new[keyword] = data_new[keyword].apply(lambda x: (x - minimun) / (maximun - minimun))\n",
        "\n",
        "keyword = 'DAY'\n",
        "maximun = data_new[keyword].max()\n",
        "minimun = data_new[keyword].min()\n",
        "data_new[keyword] = data_new[keyword].apply(lambda x: (x - minimun) / (maximun - minimun))\n",
        "\n",
        "keyword = 'DAY_OF_WEEK'\n",
        "maximun = data_new[keyword].max()\n",
        "minimun = data_new[keyword].min()\n",
        "data_new[keyword] = data_new[keyword].apply(lambda x: (x - minimun) / (maximun - minimun))\n",
        "\n",
        "keyword = 'ORIGIN_AIRPORT'\n",
        "maximun = data_new[keyword].max()\n",
        "minimun = data_new[keyword].min()\n",
        "data_new[keyword] = data_new[keyword].apply(lambda x: (x - minimun) / (maximun - minimun))\n",
        "\n",
        "keyword = 'DESTINATION_AIRPORT'\n",
        "maximun = data_new[keyword].max()\n",
        "minimun = data_new[keyword].min()\n",
        "data_new[keyword] = data_new[keyword].apply(lambda x: (x - minimun) / (maximun - minimun))\n",
        "\n",
        "keyword = 'SCHEDULED_DEPARTURE'\n",
        "maximun = data_new[keyword].max()\n",
        "minimun = data_new[keyword].min()\n",
        "data_new[keyword] = data_new[keyword].apply(lambda x: (x - minimun) / (maximun - minimun))\n",
        "\n",
        "keyword = 'ARRIVAL_DELAY'\n",
        "maximun = data_new[keyword].max()\n",
        "minimun = data_new[keyword].min()\n",
        "data_new[keyword] = data_new[keyword].apply(lambda x: (x - minimun) / (maximun - minimun))\n",
        "\n",
        "\n",
        "print(data_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUWLz1kP02NF"
      },
      "source": [
        "### 1.6 Weather API call & Merging\n",
        "API source: https://api.weatherbit.io/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJwUVXCM05fA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "airport_lib = data_new['IATA'].unique()\n",
        "airport_dict = dict()\n",
        "\n",
        "for i in airport_lib:\n",
        "  airport_dict[i] = []\n",
        "  response = requests.get(\"https://api.weatherbit.io/v2.0/history/daily?station=%s&start_date=2015-01-01&end_date=2015-02-01&key=********\" % i)\n",
        "  if response.status_code == 200:\n",
        "      response_json = response.json()\n",
        "      for j in range(31):\n",
        "        airport_dict[i].append(response_json['data'][j])\n",
        "  else:\n",
        "    print(\"API request failed\")\n",
        "\n",
        "data_new['DST_WIND_SPD'] = airport_dict['DESTINATION_AIRPORT'][data_new['DAY']]['wind_spd']\n",
        "data_new['SRC_WIND_SPD'] = airport_dict['ORIGIN_AIRPORT'][data_new['DAY']]['wind_spd']\n",
        "data_new['DST_WIND_SPD_MAX'] = airport_dict['DESTINATION_AIRPORT'][data_new['DAY']]['max_wind_spd']\n",
        "data_new['SRC_WIND_SPD_MAX'] = airport_dict['ORIGIN_AIRPORT'][data_new['DAY']]['max_wind_spd']\n",
        "data_new['DST_RH'] = airport_dict['DESTINATION_AIRPORT'][data_new['DAY']]['rh']\n",
        "data_new['SRC_RH'] = airport_dict['ORIGIN_AIRPORT'][data_new['DAY']]['rh']\n",
        "data_new['DST_CLOUD'] = airport_dict['DESTINATION_AIRPORT'][data_new['DAY']]['clouds']\n",
        "data_new['SRC_CLOUD'] = airport_dict['ORIGIN_AIRPORT'][data_new['DAY']]['clouds']\n",
        "data_new['DST_SNOW'] = airport_dict['DESTINATION_AIRPORT'][data_new['DAY']]['snow']\n",
        "data_new['SRC_SNOW'] = airport_dict['ORIGIN_AIRPORT'][data_new['DAY']]['snow']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7 Calculation Congestion Degree\n",
        "\n",
        "Definition:\n",
        "$$\n",
        "C_d =\\frac{number\\ of\\ flights\\ flying \\ towards\\ target}{max\\ acceptance\\ rate\\ of\\ the\\ airport} \n",
        "$$"
      ],
      "metadata": {
        "id": "4biFKMJ6MBA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(31):\n",
        "  for j in range(24):\n",
        "    airport_d = dict()\n",
        "    hourly = len(data[data['DAY'] == i and data['ACTUAL_DEPARTURE'] < j and data['ACTUAL_ARRIVAL'] > j])\n",
        "    data[data['DAY'] == i and data['ACTUAL_DEPARTURE'] < j and data['ACTUAL_ARRIVAL'] > j] = hourly / data['MAX_CAPACITY']"
      ],
      "metadata": {
        "id": "4QeW5yryMY28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y3NRsUlZXTl"
      },
      "source": [
        "## 2. Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 LSTM prediction\n"
      ],
      "metadata": {
        "id": "cRHn_Fr-L5GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# fix random seed for reproducibility\n",
        "tf.random.set_seed(7)\n",
        "\n",
        "# Split the DataFrame into a training set and a test set\n",
        "train_set, test_set = train_test_split(data_new, test_size=0.25)\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        " dataX, dataY = [], []\n",
        " for i in range(len(dataset)-look_back-1):\n",
        "  a = dataset[i:(i+look_back), 0]\n",
        "  dataX.append(a)\n",
        "  dataY.append(dataset[i + look_back, 0])\n",
        " return np.array(dataX), np.array(dataY)\n",
        "look_back = 1\n",
        "trainX, trainY = create_dataset(train_set, look_back)\n",
        "testX, testY = create_dataset(test_set, look_back)\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "# create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(1, look_back)))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
        "# make predictions\n",
        "testPredict = model.predict(testX)"
      ],
      "metadata": {
        "id": "r5pzdBv6L4qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUBKBod6e_dS"
      },
      "source": [
        "### 2.2 Random-forest based regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4j31EDZZW3t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use the newly-generated Cd to fill-in the test set\n",
        "test_set['CD'] = testPredict\n",
        "\n",
        "# Create a random forest regression model\n",
        "model = RandomForestRegressor(n_estimators = 100, max_depth=10)\n",
        "\n",
        "# Fit the model to the training set\n",
        "model.fit(train_set.drop(\"ARRIVAL_DELAY\", axis=1), train_set[\"ARRIVAL_DELAY\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js3_gsXRlbnQ"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set\n",
        "predictions = model.predict(test_set.drop(\"ARRIVAL_DELAY\", axis=1))\n",
        "# Evaluate the model's performance on the test set\n",
        "score = model.score(test_set.drop(\"ARRIVAL_DELAY\", axis=1), test_set[\"ARRIVAL_DELAY\"])\n",
        "actual = test_set[\"ARRIVAL_DELAY\"].values\n",
        "total = len(predictions)\n",
        "true = 0\n",
        "predictions.reshape(-1,1)\n",
        "actual.reshape(-1,1)\n",
        "for i in range(total):\n",
        "  if abs(predictions[i] - actual[i]) < 30:\n",
        "     true += 1\n",
        "print(\"True cases:\",true)\n",
        "print(\"Total cases:\",total)\n",
        "print(true / total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U901GuIcgjTi"
      },
      "source": [
        "### 2.3. Random-forest based approach (classification task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWq_1zOsggNW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRvwRg9LgoK4"
      },
      "outputs": [],
      "source": [
        "flights = pd.read_csv(\"drive/MyDrive/flights.csv\",low_memory = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNe2wQiqgqQ6"
      },
      "outputs": [],
      "source": [
        "flights_needed_data = flights[0:100000]  # getting a segment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNvA840BgsmH"
      },
      "outputs": [],
      "source": [
        "flights_needed_data.info()  # for an insight into the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MNrNyO4guGc"
      },
      "outputs": [],
      "source": [
        "flights_needed_data.value_counts('DIVERTED')  # will tell us the no. of flights which were diverted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0po4ZCqTgxXb"
      },
      "outputs": [],
      "source": [
        "sb.jointplot(data=flights_needed_data, x=\"SCHEDULED_ARRIVAL\", y=\"ARRIVAL_TIME\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_YCjmL2gyqp"
      },
      "outputs": [],
      "source": [
        "corr = flights_needed_data.corr(method='pearson')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9ALqOVmgz6L"
      },
      "outputs": [],
      "source": [
        "sb.heatmap(corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rhIHu_lg1F1"
      },
      "outputs": [],
      "source": [
        "corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZeNXNpvg2N8"
      },
      "outputs": [],
      "source": [
        "# filtering out unnecessary columns\n",
        "flights_needed_data=flights_needed_data.drop(['YEAR','FLIGHT_NUMBER','AIRLINE','DISTANCE','TAIL_NUMBER','TAXI_OUT',\n",
        "                                              'SCHEDULED_TIME','DEPARTURE_TIME','WHEELS_OFF','ELAPSED_TIME',\n",
        "                                              'AIR_TIME','WHEELS_ON','DAY_OF_WEEK','TAXI_IN','CANCELLATION_REASON'],\n",
        "                                             axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv7jWRATg3s_"
      },
      "outputs": [],
      "source": [
        "# replacing all NaN values with the mean of the attribute in which they are present\n",
        "flights_needed_data=flights_needed_data.fillna(flights_needed_data.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIPLStFJg5E4"
      },
      "outputs": [],
      "source": [
        "# creating a new column; it will tell if the flight was delayed or not\n",
        "result=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-ZSM_vVg6e2"
      },
      "outputs": [],
      "source": [
        "for row in flights_needed_data['ARRIVAL_DELAY']:\n",
        "  if row > 15:\n",
        "    result.append(1)\n",
        "  else:\n",
        "    result.append(0)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYBvUpn9g7td"
      },
      "outputs": [],
      "source": [
        "flights_needed_data['result'] = result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k_mUx2Rg816"
      },
      "outputs": [],
      "source": [
        "flights_needed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBBttFdag-PZ"
      },
      "outputs": [],
      "source": [
        "flights_needed_data.value_counts('result')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZAKh347g_Qp"
      },
      "outputs": [],
      "source": [
        "# removing some more columns\n",
        "flights_needed_data=flights_needed_data.drop(['ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'ARRIVAL_TIME', 'ARRIVAL_DELAY'],axis=1)\n",
        "flights_needed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1HnhSYehAbC"
      },
      "outputs": [],
      "source": [
        "data = flights_needed_data.values\n",
        "X, y = data[:,:-1], data[:,-1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)  # splitting in the ratio 70:30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aa3QJafhBdc"
      },
      "outputs": [],
      "source": [
        "scaled_features = StandardScaler().fit_transform(X_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yndBaffahCnH"
      },
      "outputs": [],
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SIcoXxZhDzj"
      },
      "outputs": [],
      "source": [
        "pred_prob = clf.predict_proba(X_test)\n",
        "auc_score = roc_auc_score(y_test, pred_prob[:,1])\n",
        "auc_score"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
